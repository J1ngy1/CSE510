{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE_RL_NSL-KDD.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J1ngy1/CSE510/blob/main/Notebooks/AE_RL_NSL_KDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "e091v728lBCx"
      },
      "cell_type": "markdown",
      "source": [
        "# AE-RL for NSL-KDD"
      ]
    },
    {
      "metadata": {
        "id": "FEFO4mGvbnlF"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "import json\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import sys\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YKA8QTFrb1ZR"
      },
      "cell_type": "code",
      "source": [
        "class data_cls:\n",
        "    def __init__(self,train_test,**kwargs):\n",
        "        col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "            \"dst_bytes\",\"land_f\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "            \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "            \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "            \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "            \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "            \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "            \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "            \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "            \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\",\"dificulty\"]\n",
        "        self.index = 0\n",
        "        # Data formated path and test path.\n",
        "        self.loaded = False\n",
        "        self.train_test = train_test\n",
        "        self.train_path = kwargs.get('train_path', 'datasets/NSL/KDDTrain+.txt')\n",
        "        self.test_path = kwargs.get('test_path',\n",
        "                                    'https://raw.githubusercontent.com/gcamfer/Anomaly-ReactionRL/master/datasets/NSL/KDDTest%2B.txt')\n",
        "\n",
        "        self.formated_train_path = kwargs.get('formated_train_path',\n",
        "                                              \"formated_train_adv.data\")\n",
        "        self.formated_test_path = kwargs.get('formated_test_path',\n",
        "                                             \"formated_test_adv.data\")\n",
        "\n",
        "        self.attack_types = ['normal','DoS','Probe','R2L','U2R']\n",
        "        self.attack_names = []\n",
        "        self.attack_map =   { 'normal': 'normal',\n",
        "\n",
        "                        'back': 'DoS',\n",
        "                        'land': 'DoS',\n",
        "                        'neptune': 'DoS',\n",
        "                        'pod': 'DoS',\n",
        "                        'smurf': 'DoS',\n",
        "                        'teardrop': 'DoS',\n",
        "                        'mailbomb': 'DoS',\n",
        "                        'apache2': 'DoS',\n",
        "                        'processtable': 'DoS',\n",
        "                        'udpstorm': 'DoS',\n",
        "\n",
        "                        'ipsweep': 'Probe',\n",
        "                        'nmap': 'Probe',\n",
        "                        'portsweep': 'Probe',\n",
        "                        'satan': 'Probe',\n",
        "                        'mscan': 'Probe',\n",
        "                        'saint': 'Probe',\n",
        "\n",
        "                        'ftp_write': 'R2L',\n",
        "                        'guess_passwd': 'R2L',\n",
        "                        'imap': 'R2L',\n",
        "                        'multihop': 'R2L',\n",
        "                        'phf': 'R2L',\n",
        "                        'spy': 'R2L',\n",
        "                        'warezclient': 'R2L',\n",
        "                        'warezmaster': 'R2L',\n",
        "                        'sendmail': 'R2L',\n",
        "                        'named': 'R2L',\n",
        "                        'snmpgetattack': 'R2L',\n",
        "                        'snmpguess': 'R2L',\n",
        "                        'xlock': 'R2L',\n",
        "                        'xsnoop': 'R2L',\n",
        "                        'worm': 'R2L',\n",
        "\n",
        "                        'buffer_overflow': 'U2R',\n",
        "                        'loadmodule': 'U2R',\n",
        "                        'perl': 'U2R',\n",
        "                        'rootkit': 'U2R',\n",
        "                        'httptunnel': 'U2R',\n",
        "                        'ps': 'U2R',\n",
        "                        'sqlattack': 'U2R',\n",
        "                        'xterm': 'U2R'\n",
        "                    }\n",
        "        self.all_attack_names = list(self.attack_map.keys())\n",
        "\n",
        "        formated = False\n",
        "\n",
        "        # Test formated data exists\n",
        "        if os.path.exists(self.formated_train_path) and os.path.exists(self.formated_test_path):\n",
        "            formated = True\n",
        "\n",
        "        self.formated_dir = \"../datasets/formated/\"\n",
        "        if not os.path.exists(self.formated_dir):\n",
        "            os.makedirs(self.formated_dir)\n",
        "\n",
        "\n",
        "        # If it does not exist, it's needed to format the data\n",
        "        if not formated:\n",
        "            ''' Formating the dataset for ready-2-use data'''\n",
        "            self.df = pd.read_csv(self.train_path,sep=',',names=col_names,index_col=False)\n",
        "            if 'dificulty' in self.df.columns:\n",
        "                self.df.drop('dificulty', axis=1, inplace=True) #in case of difficulty\n",
        "\n",
        "            data2 = pd.read_csv(self.test_path,sep=',',names=col_names,index_col=False)\n",
        "            if 'dificulty' in data2:\n",
        "                del(data2['dificulty'])\n",
        "            train_indx = self.df.shape[0]\n",
        "            frames = [self.df,data2]\n",
        "            self.df = pd.concat(frames)\n",
        "\n",
        "            # Dataframe processing\n",
        "            self.df = pd.concat([self.df.drop('protocol_type', axis=1), pd.get_dummies(self.df['protocol_type'])], axis=1)\n",
        "            self.df = pd.concat([self.df.drop('service', axis=1), pd.get_dummies(self.df['service'])], axis=1)\n",
        "            self.df = pd.concat([self.df.drop('flag', axis=1), pd.get_dummies(self.df['flag'])], axis=1)\n",
        "\n",
        "            # 1 if ``su root'' command attempted; 0 otherwise\n",
        "            self.df['su_attempted'] = self.df['su_attempted'].replace(2.0, 0.0)\n",
        "\n",
        "             # One hot encoding for labels\n",
        "            self.df = pd.concat([self.df.drop('labels', axis=1),\n",
        "                            pd.get_dummies(self.df['labels'])], axis=1)\n",
        "\n",
        "\n",
        "            # Normalization of the df\n",
        "            #normalized_df=(df-df.mean())/df.std()\n",
        "            for indx,dtype in self.df.dtypes.items():\n",
        "                if dtype == 'float64' or dtype == 'int64':\n",
        "                    if self.df[indx].max() == 0 and self.df[indx].min()== 0:\n",
        "                        self.df[indx] = 0\n",
        "                    else:\n",
        "                        self.df[indx] = (self.df[indx]-self.df[indx].min())/(self.df[indx].max()-self.df[indx].min())\n",
        "\n",
        "\n",
        "            # Save data\n",
        "            test_df = self.df.iloc[train_indx:self.df.shape[0]]\n",
        "            test_df = shuffle(test_df,random_state=np.random.randint(0,100))\n",
        "            self.df = self.df[:train_indx]\n",
        "            self.df = shuffle(self.df,random_state=np.random.randint(0,100))\n",
        "            test_df.to_csv(self.formated_test_path,sep=',',index=False)\n",
        "            self.df.to_csv(self.formated_train_path,sep=',',index=False)\n",
        "\n",
        "            # Create a list with the existent attacks in the df\n",
        "            for att in self.attack_map:\n",
        "                if att in self.df.columns:\n",
        "                # Add only if there is exist at least 1\n",
        "                    if np.sum(self.df[att].to_numpy()) > 1:\n",
        "                        self.attack_names.append(att)\n",
        "\n",
        "    def get_shape(self):\n",
        "        if self.loaded is False:\n",
        "            self._load_df()\n",
        "\n",
        "        self.data_shape = self.df.shape\n",
        "        # stata + labels\n",
        "        return self.data_shape\n",
        "\n",
        "    ''' Get n-rows from loaded data\n",
        "        The dataset must be loaded in RAM\n",
        "    '''\n",
        "    def get_batch(self,batch_size=100):\n",
        "        if self.loaded is False:\n",
        "            self._load_df()\n",
        "\n",
        "        # Read the df rows\n",
        "        indexes = list(range(self.index,self.index+batch_size))\n",
        "        if max(indexes)>self.data_shape[0]-1:\n",
        "            dif = max(indexes)-self.data_shape[0]\n",
        "            indexes[len(indexes)-dif-1:len(indexes)] = list(range(dif+1))\n",
        "            self.index=batch_size-dif\n",
        "            batch = self.df.iloc[indexes]\n",
        "        else:\n",
        "            batch = self.df.iloc[indexes]\n",
        "            self.index += batch_size\n",
        "\n",
        "        labels = batch[self.attack_names]\n",
        "\n",
        "        batch = batch.drop(self.all_attack_names,axis=1)\n",
        "\n",
        "        return batch,labels\n",
        "\n",
        "    def get_full(self):\n",
        "        if self.loaded is False:\n",
        "            self._load_df()\n",
        "\n",
        "\n",
        "        labels = self.df[self.attack_names]\n",
        "\n",
        "        batch = self.df.drop(self.all_attack_names,axis=1)\n",
        "\n",
        "\n",
        "        return batch,labels\n",
        "\n",
        "    def _load_df(self):\n",
        "        if self.train_test == 'train':\n",
        "            self.df = pd.read_csv(self.formated_train_path,sep=',') # Read again the csv\n",
        "        else:\n",
        "            self.df = pd.read_csv(self.formated_test_path,sep=',')\n",
        "        self.index=np.random.randint(0,self.df.shape[0]-1,dtype=np.int32)\n",
        "        self.loaded = True\n",
        "         # Create a list with the existent attacks in the df\n",
        "        for att in self.attack_map:\n",
        "            if att in self.df.columns:\n",
        "                # Add only if there is exist at least 1\n",
        "                if np.sum(self.df[att].to_numpy()) > 1:\n",
        "                    self.attack_names.append(att)\n",
        "        #self.headers = list(self.df)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5DbMxSATehU-"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def huber_loss(y_true, y_pred, clip_value=1.0):\n",
        "    \"\"\"\n",
        "    Simplified Huber Loss function.\n",
        "    Args:\n",
        "        y_true: Ground truth values.\n",
        "        y_pred: Predicted values.\n",
        "        clip_value: The threshold where the loss transitions from quadratic to linear.\n",
        "    Returns:\n",
        "        Tensor representing the computed Huber loss.\n",
        "    \"\"\"\n",
        "    assert clip_value > 0.0, \"clip_value must be positive.\"\n",
        "\n",
        "    error = y_true - y_pred\n",
        "    condition = tf.abs(error) <= clip_value\n",
        "    squared_loss = 0.5 * tf.square(error)\n",
        "    linear_loss = clip_value * (tf.abs(error) - 0.5 * clip_value)\n",
        "\n",
        "    return tf.where(condition, squared_loss, linear_loss)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XkKb6WaesLN"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "class QNetwork():\n",
        "    \"\"\"\n",
        "    Q-Network Estimator\n",
        "    Represents the global model for the table\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,obs_size,num_actions,hidden_size = 100,\n",
        "                 hidden_layers = 1,learning_rate=.2):\n",
        "        \"\"\"\n",
        "        Initialize the network with the provided shape\n",
        "        \"\"\"\n",
        "        self.obs_size = obs_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Network arquitecture\n",
        "        self.model = Sequential()\n",
        "        # Add imput layer\n",
        "        self.model.add(Dense(hidden_size, input_shape=(obs_size,),\n",
        "                             activation='relu'))\n",
        "        # Add hidden layers\n",
        "        for layers in range(hidden_layers):\n",
        "            self.model.add(Dense(hidden_size, activation='relu'))\n",
        "        # Add output layer\n",
        "        self.model.add(Dense(num_actions))\n",
        "\n",
        "        #optimizer = optimizers.SGD(learning_rate)\n",
        "        # optimizer = optimizers.Adam(alpha=learning_rate)\n",
        "        optimizer = optimizers.Adam(0.00025)\n",
        "        # optimizer = optimizers.RMSpropGraves(learning_rate, 0.95, self.momentum, 1e-2)\n",
        "\n",
        "        # Compilation of the model with optimizer and loss\n",
        "        self.model.compile(loss=huber_loss,optimizer=optimizer)\n",
        "\n",
        "    def predict(self, state, batch_size=1):\n",
        "        \"\"\"\n",
        "        Predicts action values.\n",
        "        \"\"\"\n",
        "        # Convert to NumPy array with correct dtype\n",
        "        state = np.asarray(state, dtype=np.float32)\n",
        "\n",
        "        # Check for NaN or Inf values\n",
        "        if np.any(np.isnan(state)) or np.any(np.isinf(state)):\n",
        "            raise ValueError(\"State contains NaN or Inf values. Check your input data.\")\n",
        "\n",
        "        # Reshape if it's a single sample\n",
        "        if len(state.shape) == 1:\n",
        "            state = np.expand_dims(state, axis=0)\n",
        "\n",
        "        # Debugging information\n",
        "        # print(\"State shape:\", state.shape)\n",
        "        # print(\"State dtype:\", state.dtype)\n",
        "        # print(\"Sample state data:\", state[:5])\n",
        "\n",
        "        # Predict using the model\n",
        "        return self.model.predict(state, batch_size=batch_size)\n",
        "\n",
        "    def update(self, states, q):\n",
        "        \"\"\"\n",
        "        Updates the estimator with the targets.\n",
        "\n",
        "        Args:\n",
        "          states: Target states\n",
        "          q: Estimated values\n",
        "\n",
        "        Returns:\n",
        "          The calculated loss on the batch.\n",
        "        \"\"\"\n",
        "        loss = self.model.train_on_batch(states, q)\n",
        "        return loss\n",
        "\n",
        "    def copy_model(model):\n",
        "        \"\"\"Returns a copy of a keras model.\"\"\"\n",
        "        model.save('tmp_model.keras')\n",
        "        return keras.models.load_model('tmp_model.keras', custom_objects={'huber_loss': tf.keras.losses.huber})\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-u8-r77Jexf2"
      },
      "cell_type": "code",
      "source": [
        "#Policy interface\n",
        "class Policy:\n",
        "    def __init__(self, num_actions, estimator):\n",
        "        self.num_actions = num_actions\n",
        "        self.estimator = estimator\n",
        "\n",
        "class Epsilon_greedy(Policy):\n",
        "    def __init__(self,estimator ,num_actions ,epsilon,min_epsilon,decay_rate, epoch_length):\n",
        "        Policy.__init__(self, num_actions, estimator)\n",
        "        self.name = \"Epsilon Greedy\"\n",
        "\n",
        "        if (epsilon is None or epsilon < 0 or epsilon > 1):\n",
        "            print(\"EpsilonGreedy: Invalid value of epsilon\", flush = True)\n",
        "            sys.exit(0)\n",
        "        self.epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.actions = list(range(num_actions))\n",
        "        self.step_counter = 0\n",
        "        self.epoch_length = epoch_length\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "        #if epsilon is up 0.1, it will be decayed over time\n",
        "        if self.epsilon > 0.01:\n",
        "            self.epsilon_decay = True\n",
        "        else:\n",
        "            self.epsilon_decay = False\n",
        "\n",
        "    def get_actions(self,states):\n",
        "        # get next action\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            actions = np.random.randint(0, self.num_actions,states.shape[0])\n",
        "        else:\n",
        "            self.Q = self.estimator.predict(states,states.shape[0])\n",
        "            actions = []\n",
        "            for row in range(self.Q.shape[0]):\n",
        "                best_actions = np.argwhere(self.Q[row] == np.amax(self.Q[row]))\n",
        "                actions.append(best_actions[np.random.choice(len(best_actions))].item())\n",
        "\n",
        "        self.step_counter += 1\n",
        "        # decay epsilon after each epoch\n",
        "        if self.epsilon_decay:\n",
        "            if self.step_counter % self.epoch_length == 0:\n",
        "                self.epsilon = max(self.min_epsilon, self.epsilon * self.decay_rate**self.step_counter)\n",
        "\n",
        "        return actions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2gBTfI8ke08v"
      },
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "    \"\"\"Implements basic replay memory\"\"\"\n",
        "\n",
        "    def __init__(self, observation_size, max_size):\n",
        "        self.observation_size = observation_size\n",
        "        self.num_observed = 0\n",
        "        self.max_size = max_size\n",
        "        self.samples = {\n",
        "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
        "                                       dtype=np.float32).reshape(self.max_size,self.observation_size),\n",
        "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
        "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
        "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
        "               }\n",
        "\n",
        "    def observe(self, state, action, reward, done):\n",
        "        index = self.num_observed % self.max_size\n",
        "        self.samples['obs'][index, :] = state\n",
        "        self.samples['action'][index, :] = action\n",
        "        self.samples['reward'][index, :] = reward\n",
        "        self.samples['terminal'][index, :] = done\n",
        "\n",
        "        self.num_observed += 1\n",
        "\n",
        "    def sample_minibatch(self, minibatch_size):\n",
        "        max_index = min(self.num_observed, self.max_size) - 1\n",
        "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
        "\n",
        "        s      = np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32)\n",
        "        s_next = np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32)\n",
        "\n",
        "        a      = self.samples['action'][sampled_indices].reshape(minibatch_size)\n",
        "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
        "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
        "\n",
        "        return (s, a, r, s_next, done)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXeEzOaIe32m"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Reinforcement learning Agent definition\n",
        "'''\n",
        "\n",
        "class Agent(object):\n",
        "\n",
        "    def __init__(self, actions,obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
        "        self.actions = actions\n",
        "        self.num_actions = len(actions)\n",
        "        self.obs_size = obs_size\n",
        "\n",
        "        self.epsilon = kwargs.get('epsilon', 1)\n",
        "        self.min_epsilon = kwargs.get('min_epsilon', .1)\n",
        "        self.gamma = kwargs.get('gamma', .001)\n",
        "        self.minibatch_size = kwargs.get('minibatch_size', 2)\n",
        "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
        "        self.decay_rate = kwargs.get('decay_rate',0.99)\n",
        "        self.ExpRep = kwargs.get('ExpRep',True)\n",
        "        if self.ExpRep:\n",
        "            self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 10))\n",
        "\n",
        "        self.ddqn_time = 100\n",
        "        self.ddqn_update = self.ddqn_time\n",
        "\n",
        "\n",
        "        self.model_network = QNetwork(self.obs_size, self.num_actions,\n",
        "                                      kwargs.get('hidden_size', 100),\n",
        "                                      kwargs.get('hidden_layers',1),\n",
        "                                      kwargs.get('learning_rate',.2))\n",
        "        self.target_model_network = QNetwork(self.obs_size, self.num_actions,\n",
        "                                      kwargs.get('hidden_size', 100),\n",
        "                                      kwargs.get('hidden_layers',1),\n",
        "                                      kwargs.get('learning_rate',.2))\n",
        "        self.target_model_network.model = QNetwork.copy_model(self.model_network.model)\n",
        "\n",
        "        if policy == \"EpsilonGreedy\":\n",
        "            self.policy = Epsilon_greedy(self.model_network,len(actions),\n",
        "                                         self.epsilon,self.min_epsilon,\n",
        "                                         self.decay_rate,self.epoch_length)\n",
        "\n",
        "\n",
        "    def learn(self, states, actions,next_states, rewards, done):\n",
        "        if self.ExpRep:\n",
        "            self.memory.observe(states, actions, rewards, done)\n",
        "        else:\n",
        "            self.states = states\n",
        "            self.actions = actions\n",
        "            self.next_states = next_states\n",
        "            self.rewards = rewards\n",
        "            self.done = done\n",
        "    def update_model(self):\n",
        "        if self.ExpRep:\n",
        "            (states, actions, rewards, next_states, done) = self.memory.sample_minibatch(self.minibatch_size)\n",
        "        else:\n",
        "            states = self.states\n",
        "            rewards = self.rewards\n",
        "            next_states = self.next_states\n",
        "            actions = self.actions\n",
        "            done = self.done\n",
        "\n",
        "        next_actions = []\n",
        "        # Compute Q targets\n",
        "#        Q_prime = self.model_network.predict(next_states,self.minibatch_size)\n",
        "        Q_prime = self.target_model_network.predict(next_states,self.minibatch_size)\n",
        "        # TODO: fix performance in this loop\n",
        "        for row in range(Q_prime.shape[0]):\n",
        "            best_next_actions = np.argwhere(Q_prime[row] == np.amax(Q_prime[row]))\n",
        "            next_actions.append(best_next_actions[np.random.choice(len(best_next_actions))].item())\n",
        "        sx = np.arange(len(next_actions))\n",
        "        # Compute Q(s,a)\n",
        "        Q = self.model_network.predict(states,self.minibatch_size)\n",
        "        # Q-learning update\n",
        "        # target = reward + gamma * max_a'{Q(next_state,next_action))}\n",
        "        targets = rewards.reshape(Q[sx,actions].shape) + \\\n",
        "                  self.gamma * Q[sx,next_actions] * \\\n",
        "                  (1-done.reshape(Q[sx,actions].shape))\n",
        "        Q[sx,actions] = targets\n",
        "\n",
        "        loss = self.model_network.model.train_on_batch(states,Q)#inputs,targets\n",
        "\n",
        "        # timer to ddqn update\n",
        "        self.ddqn_update -= 1\n",
        "        if self.ddqn_update == 0:\n",
        "            self.ddqn_update = self.ddqn_time\n",
        "#            self.target_model_network.model = QNetwork.copy_model(self.model_network.model)\n",
        "            self.target_model_network.model.set_weights(self.model_network.model.get_weights())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def act(self, state,policy):\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-K4eEuDe-QE"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzoWE1L8fCKP"
      },
      "cell_type": "code",
      "source": [
        "class DefenderAgent(Agent):\n",
        "    def __init__(self, actions, obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
        "        super().__init__(actions,obs_size, policy=\"EpsilonGreedy\", **kwargs)\n",
        "\n",
        "    def act(self,states):\n",
        "        # Get actions under the policy\n",
        "        actions = self.policy.get_actions(states)\n",
        "        return actions\n",
        "\n",
        "class AttackAgent(Agent):\n",
        "    def __init__(self, actions, obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
        "        super().__init__(actions,obs_size, policy=\"EpsilonGreedy\", **kwargs)\n",
        "\n",
        "    def act(self,states):\n",
        "        # Get actions under the policy\n",
        "        actions = self.policy.get_actions(states)\n",
        "        return actions"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ojLCmmE3fCpw"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Reinforcement learning Enviroment Definition\n",
        "'''\n",
        "class RLenv(data_cls):\n",
        "    def __init__(self,train_test,**kwargs):\n",
        "        data_cls.__init__(self,train_test,**kwargs)\n",
        "        data_cls._load_df(self)\n",
        "        self.data_shape = data_cls.get_shape(self)\n",
        "        self.batch_size = kwargs.get('batch_size',1) # experience replay -> batch = 1\n",
        "        self.iterations_episode = kwargs.get('iterations_episode',10)\n",
        "        if self.batch_size=='full':\n",
        "            self.batch_size = int(self.data_shape[0]/iterations_episode)\n",
        "\n",
        "\n",
        "    '''\n",
        "    _update_state: function to update the current state\n",
        "    Returns:\n",
        "        None\n",
        "    Modifies the self parameters involved in the state:\n",
        "        self.state and self.labels\n",
        "    Also modifies the true labels to get learning knowledge\n",
        "    '''\n",
        "    def _update_state(self):\n",
        "        self.states,self.labels = data_cls.get_batch(self)\n",
        "\n",
        "        # Update statistics\n",
        "        self.true_labels += np.sum(self.labels).to_numpy()\n",
        "\n",
        "    '''\n",
        "    Returns:\n",
        "        + Observation of the enviroment\n",
        "    '''\n",
        "    def reset(self):\n",
        "        # Statistics\n",
        "        self.def_true_labels = np.zeros(len(self.attack_types),dtype=int)\n",
        "        self.def_estimated_labels = np.zeros(len(self.attack_types),dtype=int)\n",
        "        self.att_true_labels = np.zeros(len(self.attack_names),dtype=int)\n",
        "\n",
        "        self.state_numb = 0\n",
        "\n",
        "        data_cls._load_df(self) # Reload and random index\n",
        "        self.states,self.labels = data_cls.get_batch(self,self.batch_size)\n",
        "\n",
        "        self.total_reward = 0\n",
        "        self.steps_in_episode = 0\n",
        "        return self.states.to_numpy()\n",
        "\n",
        "    '''\n",
        "    Returns:\n",
        "        State: Next state for the game\n",
        "        Reward: Actual reward\n",
        "        done: If the game ends (no end in this case)\n",
        "\n",
        "    In the adversarial enviroment, it's only needed to return the actual reward\n",
        "    '''\n",
        "    def act(self,defender_actions,attack_actions):\n",
        "        # Clear previous rewards\n",
        "        self.att_reward = np.zeros(len(attack_actions))\n",
        "        self.def_reward = np.zeros(len(defender_actions))\n",
        "\n",
        "\n",
        "        attack = [self.attack_types.index(self.attack_map[self.attack_names[att]]) for att in attack_actions]\n",
        "\n",
        "        self.def_reward = (np.asarray(defender_actions)==np.asarray(attack))*1\n",
        "        self.att_reward = (np.asarray(defender_actions)!=np.asarray(attack))*1\n",
        "\n",
        "\n",
        "\n",
        "        self.def_estimated_labels += np.bincount(defender_actions,minlength=len(self.attack_types))\n",
        "        # TODO\n",
        "        # list comprehension\n",
        "\n",
        "        for act in attack_actions:\n",
        "            self.def_true_labels[self.attack_types.index(self.attack_map[self.attack_names[act]])] += 1\n",
        "\n",
        "\n",
        "        # Get new state and new true values\n",
        "        attack_actions = attacker_agent.act(self.states)\n",
        "        self.states = env.get_states(attack_actions)\n",
        "\n",
        "        # Done allways false in this continuous task\n",
        "        self.done = np.zeros(len(attack_actions),dtype=bool)\n",
        "\n",
        "        return self.states, self.def_reward,self.att_reward, attack_actions, self.done\n",
        "\n",
        "    '''\n",
        "    Provide the actual states for the selected attacker actions\n",
        "    Parameters:\n",
        "        self:\n",
        "        attacker_actions: optimum attacks selected by the attacker\n",
        "            it can be one of attack_names list and select random of this\n",
        "    Returns:\n",
        "        State: Actual state for the selected attacks\n",
        "    '''\n",
        "    def get_states(self,attacker_actions):\n",
        "        first = True\n",
        "        for attack in attacker_actions:\n",
        "            if first:\n",
        "                minibatch = (self.df[self.df[self.attack_names[attack]]==1].sample(1))\n",
        "                first = False\n",
        "            else:\n",
        "                minibatch = pd.concat([minibatch, self.df[self.df[self.attack_names[attack]] == 1].sample(1)])\n",
        "\n",
        "        self.labels = minibatch[self.attack_names]\n",
        "        minibatch.drop(self.all_attack_names,axis=1,inplace=True)\n",
        "        self.states = minibatch\n",
        "\n",
        "        return self.states\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0eU7ZSGfNEB",
        "outputId": "3e32ec6f-6217-4470-d55c-3fe8e4c94119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    kdd_train = \"https://raw.githubusercontent.com/gcamfer/Anomaly-ReactionRL/master/datasets/NSL/KDDTrain%2B.txt\"\n",
        "    kdd_test = \"https://raw.githubusercontent.com/gcamfer/Anomaly-ReactionRL/master/datasets/NSL/KDDTest%2B.txt\"\n",
        "\n",
        "    formated_train_path = \"formated_train_adv.data\"\n",
        "    formated_test_path = \"formated_test_adv.data\"\n",
        "\n",
        "\n",
        "    # Train batch\n",
        "    batch_size = 1\n",
        "    # batch of memory ExpRep\n",
        "    minibatch_size = 100\n",
        "    ExpRep = True\n",
        "\n",
        "    iterations_episode = 10\n",
        "\n",
        "    # Initialization of the enviroment\n",
        "    env = RLenv('train',train_path=kdd_train,test_path=kdd_test,\n",
        "                formated_train_path = formated_train_path,\n",
        "                formated_test_path = formated_test_path,batch_size=batch_size,\n",
        "                iterations_episode=iterations_episode)\n",
        "    # obs_size = size of the state\n",
        "    obs_size = env.data_shape[1]-len(env.all_attack_names)\n",
        "\n",
        "    #num_episodes = int(env.data_shape[0]/(iterations_episode)/10)\n",
        "    num_episodes = 10\n",
        "\n",
        "    '''\n",
        "    Definition for the defensor agent.\n",
        "    '''\n",
        "    defender_valid_actions = list(range(len(env.attack_types))) # only detect type of attack\n",
        "    defender_num_actions = len(defender_valid_actions)\n",
        "\n",
        "\n",
        "    def_epsilon = 1 # exploration\n",
        "    min_epsilon = 0.01 # min value for exploration\n",
        "    def_gamma = 0.001\n",
        "    def_decay_rate = 0.99\n",
        "\n",
        "    def_hidden_size = 100\n",
        "    def_hidden_layers = 3\n",
        "\n",
        "    def_learning_rate = .2\n",
        "\n",
        "    defender_agent = DefenderAgent(defender_valid_actions,obs_size,\"EpsilonGreedy\",\n",
        "                          epoch_length = iterations_episode,\n",
        "                          epsilon = def_epsilon,\n",
        "                          min_epsilon = min_epsilon,\n",
        "                          decay_rate = def_decay_rate,\n",
        "                          gamma = def_gamma,\n",
        "                          hidden_size=def_hidden_size,\n",
        "                          hidden_layers=def_hidden_layers,\n",
        "                          minibatch_size = minibatch_size,\n",
        "                          mem_size = 1000,\n",
        "                          learning_rate=def_learning_rate,\n",
        "                          ExpRep=ExpRep)\n",
        "    #Pretrained defender\n",
        "    #defender_agent.model_network.model.load_weights(\"models/type_model.h5\")\n",
        "\n",
        "    '''\n",
        "    Definition for the attacker agent.\n",
        "    In this case the exploration is better to be greater\n",
        "    The correlation sould be greater too so gamma bigger\n",
        "    '''\n",
        "    attack_valid_actions = list(range(len(env.attack_names)))\n",
        "    attack_num_actions = len(attack_valid_actions)\n",
        "\n",
        "    att_epsilon = 1\n",
        "    min_epsilon = 0.82 # min value for exploration\n",
        "\n",
        "    att_gamma = 0.001\n",
        "    att_decay_rate = 0.99\n",
        "\n",
        "    att_hidden_layers = 1\n",
        "    att_hidden_size = 100\n",
        "\n",
        "    att_learning_rate = 0.2\n",
        "\n",
        "    attacker_agent = AttackAgent(attack_valid_actions,obs_size,\"EpsilonGreedy\",\n",
        "                          epoch_length = iterations_episode,\n",
        "                          epsilon = att_epsilon,\n",
        "                          min_epsilon = min_epsilon,\n",
        "                          decay_rate = att_decay_rate,\n",
        "                          gamma = att_gamma,\n",
        "                          hidden_size=att_hidden_size,\n",
        "                          hidden_layers=att_hidden_layers,\n",
        "                          minibatch_size = minibatch_size,\n",
        "                          mem_size = 1000,\n",
        "                          learning_rate=att_learning_rate,\n",
        "                          ExpRep=ExpRep)\n",
        "\n",
        "\n",
        "\n",
        "    # Statistics\n",
        "    att_reward_chain = []\n",
        "    def_reward_chain = []\n",
        "    att_loss_chain = []\n",
        "    def_loss_chain = []\n",
        "    def_total_reward_chain = []\n",
        "    att_total_reward_chain = []\n",
        "\n",
        "\t# Print parameters\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(\"Total epoch: {} | Iterations in epoch: {}\"\n",
        "          \"| Minibatch from mem size: {} | Total Samples: {}|\".format(num_episodes,\n",
        "                         iterations_episode,minibatch_size,\n",
        "                         num_episodes*iterations_episode))\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(\"Dataset shape: {}\".format(env.data_shape))\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(\"Attacker parameters: Num_actions={} | gamma={} |\"\n",
        "          \" epsilon={} | ANN hidden size={} | \"\n",
        "          \"ANN hidden layers={}|\".format(attack_num_actions,\n",
        "                             att_gamma,att_epsilon, att_hidden_size,\n",
        "                             att_hidden_layers))\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(\"Defense parameters: Num_actions={} | gamma={} | \"\n",
        "          \"epsilon={} | ANN hidden size={} |\"\n",
        "          \" ANN hidden layers={}|\".format(defender_num_actions,\n",
        "                              def_gamma,def_epsilon,def_hidden_size,\n",
        "                              def_hidden_layers))\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Main loop\n",
        "    attacks_by_epoch = []\n",
        "    attack_labels_list = []\n",
        "    for epoch in range(num_episodes):\n",
        "        start_time = time.time()\n",
        "        att_loss = 0.\n",
        "        def_loss = 0.\n",
        "        def_total_reward_by_episode = 0\n",
        "        att_total_reward_by_episode = 0\n",
        "        # Reset enviromet, actualize the data batch with random state/attacks\n",
        "        states = env.reset()\n",
        "\n",
        "        # Get actions for actual states following the policy\n",
        "        attack_actions = attacker_agent.act(states)\n",
        "        states = env.get_states(attack_actions)\n",
        "\n",
        "        done = False\n",
        "\n",
        "        attacks_list = []\n",
        "        # Iteration in one episode\n",
        "        for i_iteration in range(iterations_episode):\n",
        "\n",
        "            attacks_list.append(attack_actions[0])\n",
        "            # apply actions, get rewards and new state\n",
        "            act_time = time.time()\n",
        "            defender_actions = defender_agent.act(states)\n",
        "            #Enviroment actuation for this actions\n",
        "            next_states,def_reward, att_reward,next_attack_actions, done = env.act(defender_actions,attack_actions)\n",
        "            # If the epoch*batch_size*iterations_episode is largest than the df\n",
        "\n",
        "\n",
        "            attacker_agent.learn(states,attack_actions,next_states,att_reward,done)\n",
        "            defender_agent.learn(states,defender_actions,next_states,def_reward,done)\n",
        "\n",
        "            act_end_time = time.time()\n",
        "\n",
        "            # Train network, update loss after at least minibatch_learns\n",
        "            if ExpRep and epoch*iterations_episode + i_iteration >= minibatch_size:\n",
        "                def_loss += defender_agent.update_model()\n",
        "                att_loss += attacker_agent.update_model()\n",
        "            elif not ExpRep:\n",
        "                def_loss += defender_agent.update_model()\n",
        "                att_loss += attacker_agent.update_model()\n",
        "\n",
        "\n",
        "            update_end_time = time.time()\n",
        "\n",
        "            # Update the state\n",
        "            states = next_states\n",
        "            attack_actions = next_attack_actions\n",
        "\n",
        "\n",
        "            # Update statistics\n",
        "            def_total_reward_by_episode += np.sum(def_reward,dtype=np.int32)\n",
        "            att_total_reward_by_episode += np.sum(att_reward,dtype=np.int32)\n",
        "\n",
        "        attacks_by_epoch.append(attacks_list)\n",
        "        # Update user view\n",
        "        def_reward_chain.append(def_total_reward_by_episode)\n",
        "        att_reward_chain.append(att_total_reward_by_episode)\n",
        "        def_loss_chain.append(def_loss)\n",
        "        att_loss_chain.append(att_loss)\n",
        "\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(\"\\r\\n|Epoch {:03d}/{:03d}| time: {:2.2f}|\\r\\n\"\n",
        "                \"|Def Loss {:4.4f} | Def Reward in ep {:03d}|\\r\\n\"\n",
        "                \"|Att Loss {:4.4f} | Att Reward in ep {:03d}|\"\n",
        "                .format(epoch, num_episodes,(end_time-start_time),\n",
        "                def_loss, def_total_reward_by_episode,\n",
        "                att_loss, att_total_reward_by_episode))\n",
        "\n",
        "\n",
        "        print(\"|Def Estimated: {}| Att Labels: {}\".format(env.def_estimated_labels,\n",
        "              env.def_true_labels))\n",
        "        attack_labels_list.append(env.def_true_labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "Total epoch: 100 | Iterations in epoch: 100| Minibatch from mem size: 100 | Total Samples: 10000|\n",
            "-------------------------------------------------------------------------------\n",
            "Dataset shape: (125973, 162)\n",
            "-------------------------------------------------------------------------------\n",
            "Attacker parameters: Num_actions=46 | gamma=0.001 | epsilon=1 | ANN hidden size=100 | ANN hidden layers=1|\n",
            "-------------------------------------------------------------------------------\n",
            "Defense parameters: Num_actions=5 | gamma=0.001 | epsilon=1 | ANN hidden size=100 | ANN hidden layers=3|\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            "|Epoch 000/100| time: 9.19|\n",
            "|Def Loss 0.0000 | Def Reward in ep 022|\n",
            "|Att Loss 0.0000 | Att Reward in ep 078|\n",
            "|Def Estimated: [19 13 23 25 20]| Att Labels: [ 8 27 13 38 14]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b2c99b12d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b2c94f85e10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7b2c99b113f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7b2c94f86d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-802332ec1f7f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mExpRep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterations_episode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi_iteration\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdef_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefender_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0matt_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mattacker_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mExpRep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mdef_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefender_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0acb00577b15>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Compute Q targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#        Q_prime = self.model_network.predict(next_states,self.minibatch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mQ_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m# TODO: fix performance in this loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_prime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-432744e9a7a8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state, batch_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Predict using the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    448\u001b[0m     ):\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Create an iterator that yields batches of input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2309\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2312\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m       warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m     36\u001b[0m                     \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m---> 37\u001b[0;31m     return _MapDataset(\n\u001b[0m\u001b[1;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    111\u001b[0m         use_legacy_function=use_legacy_function)\n\u001b[1;32m    112\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   3527\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3528\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3529\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3530\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MapDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3531\u001b[0m         \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w56BPg3zo0DC"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2QzwrilomWO"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoBlm8f7fW_C"
      },
      "cell_type": "code",
      "source": [
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "# Save trained model weights and architecture, used in test\n",
        "defender_agent.model_network.model.save_weights(\"models/defender_agent_model.weights.h5\", overwrite=True)\n",
        "with open(\"models/defender_agent_model.json\", \"w\") as outfile:\n",
        "    json.dump(defender_agent.model_network.model.to_json(), outfile)\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "# Plot training results\n",
        "plt.figure(1)\n",
        "plt.subplot(211)\n",
        "plt.plot(np.arange(len(def_reward_chain)),def_reward_chain,label='Defense')\n",
        "plt.plot(np.arange(len(att_reward_chain)),att_reward_chain,label='Attack')\n",
        "plt.title('Total reward by episode')\n",
        "plt.xlabel('n Episode')\n",
        "plt.ylabel('Total reward')\n",
        "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
        "       ncol=2, mode=\"expand\", borderaxespad=0.)\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(np.arange(len(def_loss_chain)),def_loss_chain,label='Defense')\n",
        "plt.plot(np.arange(len(att_loss_chain)),att_loss_chain,label='Attack')\n",
        "plt.title('Loss by episode')\n",
        "plt.xlabel('n Episode')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
        "       ncol=2, mode=\"expand\", borderaxespad=0.)\n",
        "plt.tight_layout()\n",
        "#plt.show()\n",
        "plt.savefig('results/train_adv.eps', format='eps', dpi=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GdEEgnejoANJ"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MLhFt4v2oA5r"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3z12k-CoBqI"
      },
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ]
    },
    {
      "metadata": {
        "id": "k2EScB5ZoA8t"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import  confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1PgEGysoOlU"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5cSeFJb3oPVT"
      },
      "cell_type": "code",
      "source": [
        "formated_test_path = \"formated_test_adv.data\"\n",
        "\n",
        "with open(\"models/defender_agent_model.json\", \"r\") as jfile:\n",
        "    model = model_from_json(json.load(jfile),custom_objects={'huber_loss': huber_loss})\n",
        "model.load_weights(\"models/defender_agent_model.weights.h5\")\n",
        "\n",
        "model.compile(loss=huber_loss,optimizer=\"sgd\")\n",
        "\n",
        "\n",
        "# Define environment, game, make sure the batch_size is the same in train\n",
        "env_test = RLenv('test',formated_test_path = formated_test_path)\n",
        "\n",
        "\n",
        "total_reward = 0\n",
        "\n",
        "\n",
        "true_labels = np.zeros(len(env_test.attack_types),dtype=int)\n",
        "estimated_labels = np.zeros(len(env_test.attack_types),dtype=int)\n",
        "estimated_correct_labels = np.zeros(len(env_test.attack_types),dtype=int)\n",
        "\n",
        "#states , labels = env.get_sequential_batch(test_path,batch_size = env.batch_size)\n",
        "states , labels = env_test.get_full()\n",
        "\n",
        "\n",
        "start_time=time.time()\n",
        "q = model.predict(states)\n",
        "actions = np.argmax(q,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hFU1HF1oYFG"
      },
      "cell_type": "code",
      "source": [
        "maped=[]\n",
        "for indx,label in labels.iterrows():\n",
        "    maped.append(env_test.attack_types.index(env_test.attack_map[label.idxmax()]))\n",
        "\n",
        "labels,counts = np.unique(maped,return_counts=True)\n",
        "true_labels[labels] += counts\n",
        "\n",
        "\n",
        "\n",
        "for indx,a in enumerate(actions):\n",
        "    estimated_labels[a] +=1\n",
        "    if a == maped[indx]:\n",
        "        total_reward += 1\n",
        "        estimated_correct_labels[a] += 1\n",
        "\n",
        "\n",
        "action_dummies = pd.get_dummies(actions)\n",
        "posible_actions = np.arange(len(env_test.attack_types))\n",
        "for non_existing_action in posible_actions:\n",
        "    if non_existing_action not in action_dummies.columns:\n",
        "        action_dummies[non_existing_action] = np.uint8(0)\n",
        "labels_dummies = pd.get_dummies(maped)\n",
        "\n",
        "normal_f1_score = f1_score(labels_dummies[0].to_numpy(),action_dummies[0].to_numpy())\n",
        "dos_f1_score = f1_score(labels_dummies[1].to_numpy(),action_dummies[1].to_numpy())\n",
        "probe_f1_score = f1_score(labels_dummies[2].to_numpy(),action_dummies[2].to_numpy())\n",
        "r2l_f1_score = f1_score(labels_dummies[3].to_numpy(),action_dummies[3].to_numpy())\n",
        "u2r_f1_score = f1_score(labels_dummies[4].to_numpy(),action_dummies[4].to_numpy())\n",
        "\n",
        "\n",
        "Accuracy = [normal_f1_score,dos_f1_score,probe_f1_score,r2l_f1_score,u2r_f1_score]\n",
        "Mismatch = estimated_labels - true_labels\n",
        "\n",
        "acc = float(100*total_reward/len(states))\n",
        "print('\\r\\nTotal reward: {} | Number of samples: {} | Accuracy = {:.2f}%'.format(total_reward,\n",
        "      len(states),acc))\n",
        "outputs_df = pd.DataFrame(index = env_test.attack_types,columns = [\"Estimated\",\"Correct\",\"Total\",\"F1_score\"])\n",
        "for indx,att in enumerate(env_test.attack_types):\n",
        "   outputs_df.iloc[indx].Estimated = estimated_labels[indx]\n",
        "   outputs_df.iloc[indx].Correct = estimated_correct_labels[indx]\n",
        "   outputs_df.iloc[indx].Total = true_labels[indx]\n",
        "   outputs_df.iloc[indx].F1_score = Accuracy[indx]*100\n",
        "   outputs_df.iloc[indx].Mismatch = abs(Mismatch[indx])\n",
        "\n",
        "\n",
        "print(outputs_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBxETWRcocbS"
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Define bar width and positions\n",
        "bar_width = 0.35\n",
        "positions = np.arange(len(actual_labels))  # Assuming `true_labels` is renamed to `actual_labels`\n",
        "\n",
        "# Plot bars for correct estimations\n",
        "bar_correct = ax.bar(\n",
        "    positions,\n",
        "    correctly_estimated_labels,  # Renamed `estimated_correct_labels`\n",
        "    width=bar_width,\n",
        "    color='green',\n",
        "    label='Correct Predictions'\n",
        ")\n",
        "\n",
        "# Plot bars for false negatives\n",
        "bar_false_negatives = ax.bar(\n",
        "    positions + bar_width,\n",
        "    np.abs(correctly_estimated_labels - actual_labels),\n",
        "    width=bar_width,\n",
        "    color='red',\n",
        "    label='False Negatives'\n",
        ")\n",
        "\n",
        "# Plot bars for false positives\n",
        "bar_false_positives = ax.bar(\n",
        "    positions + bar_width,\n",
        "    np.abs(total_estimated_labels - correctly_estimated_labels),  # Renamed `estimated_labels`\n",
        "    width=bar_width,\n",
        "    bottom=np.abs(correctly_estimated_labels - actual_labels),\n",
        "    color='blue',\n",
        "    label='False Positives'\n",
        ")\n",
        "\n",
        "# Configure axis settings\n",
        "ax.yaxis.set_tick_params(labelsize=15)\n",
        "ax.set_xticks(positions + bar_width / 2)\n",
        "ax.set_xticklabels(env.attack_types, rotation='vertical', fontsize='xx-large')  # Match dataset context\n",
        "\n",
        "# Add legend\n",
        "ax.legend(fontsize='x-large')\n",
        "\n",
        "# Tight layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot as an SVG file\n",
        "plt.savefig('results/test_results_nsl_kdd.svg', format='svg', dpi=1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FBVTk9aujCuI"
      },
      "cell_type": "code",
      "source": [
        "estimated_correct_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UphioXJOjSpn"
      },
      "cell_type": "code",
      "source": [
        "estimated_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XrjAwLzjbWf"
      },
      "cell_type": "code",
      "source": [
        "true_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jw9xTn9EkMku"
      },
      "cell_type": "code",
      "source": [
        "np.abs(estimated_correct_labels-true_labels) # false negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSneeTpxkYiU"
      },
      "cell_type": "code",
      "source": [
        "np.abs(estimated_labels-estimated_correct_labels)  #false positive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O0Z8IHZhonlP"
      },
      "cell_type": "code",
      "source": [
        "aggregated_data_test = np.array(maped)\n",
        "\n",
        "print('Performance measures on Test data')\n",
        "print('Accuracy =  {:.4f}'.format(accuracy_score( aggregated_data_test,actions)))\n",
        "print('F1 =  {:.4f}'.format(f1_score(aggregated_data_test,actions, average='weighted')))\n",
        "print('Precision_score =  {:.4f}'.format(precision_score(aggregated_data_test,actions, average='weighted')))\n",
        "print('recall_score =  {:.4f}'.format(recall_score(aggregated_data_test,actions, average='weighted')))\n",
        "\n",
        "cnf_matrix = confusion_matrix(aggregated_data_test,actions)\n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure()\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=env.attack_types, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "plt.savefig('results/confusion_matrix_adversarial.svg', format='svg', dpi=1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "60L3BNptopsZ"
      },
      "cell_type": "code",
      "source": [
        "mapa = {0:'normal', 1:'DoS', 2:'Probe',3:'R2L',4:'U2R'}\n",
        "yt_app = pd.Series(maped).map(mapa)\n",
        "\n",
        "perf_per_class = pd.DataFrame(index=range(len(yt_app.unique())),columns=['name', 'acc','f1', 'pre','rec'])\n",
        "for i,x in enumerate(pd.Series(yt_app).value_counts().index):\n",
        "    y_test_hat_check = pd.Series(actions).map(mapa).copy()\n",
        "    y_test_hat_check[y_test_hat_check != x] = 'OTHER'\n",
        "    yt_app = pd.Series(maped).map(mapa).copy()\n",
        "    yt_app[yt_app != x] = 'OTHER'\n",
        "    ac=accuracy_score( yt_app,y_test_hat_check)\n",
        "    f1=f1_score( yt_app,y_test_hat_check,pos_label=x, average='binary')\n",
        "    pr=precision_score( yt_app,y_test_hat_check,pos_label=x, average='binary')\n",
        "    re=recall_score( yt_app,y_test_hat_check,pos_label=x, average='binary')\n",
        "    perf_per_class.iloc[i]=[x,ac,f1,pr,re]\n",
        "\n",
        "print(\"\\r\\nOne vs All metrics: \\r\\n{}\".format(perf_per_class))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-i1UuwC8Zz_b"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}